% Section 2, Subsection 1
\subsection*{Terminology \& Modeling Assumptions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 2, Subsection 1(a)

\textbf{Random Variables \& Independence}

Consider some arbitrary \texttt{mass.gov} users, Jack from Worcester and Bob from Dorchester. Suppose that both Jack and Bob are browsing through a set of pages on \texttt{mass.gov} and each submits a response to a Formstack survey. We deem each response as \href{https://en.wikipedia.org/wiki/Independence_(probability_theory)}{independent} of the other. That is, whether Jack chooses \textit{Yes} or \textit{No} to the question \textit{Did you find the information you were looking for on this page?} has no effect on the response chosen by Bob. We generalize this assumption to all site users who submit a Formstack survey response.

In our model, we make use of \href{https://en.wikipedia.org/wiki/Random_variable}{random variables}, which are functions of the set of outcomes for a random experiment and often count the occurrences of a certain outcome. A \href{https://en.wikipedia.org/wiki/Experiment_(probability_theory)}{random experiment} is a set of trials for some reproducible process with a clearly defined set of possible outcomes, such as a sequence of coin tosses in which we count the number of times we observe \textit{Heads}. In our case, the random experiment would take the form of Formstack surveys on a particular webpage or group of webpages, where each response by a user is a trial. Recall that these trials are assumed to be independent of one another. Reducing the set of outcomes to \textit{Yes} or \textit{No} responses, we see that describing observed outcomes across survey responses is similar to counting the number of \textit{Heads} or \textit{Tails} in a sequence of coin flips. In either case, we would count the number of times we observe a certain result, such as affirmative responses or \textit{Heads}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 2, Subsection 1(a) - Example BEGIN

\textbf{. . . . .}

We typically model coin toss experiments with random variables, typically denoted by upper-case letters. Suppose we have a fair (unbiased) coin and we want to count the number of times we toss \textit{Heads} over $n$ successive trials, for some positive integer $n$. Let $X$ be a random variable representing the number of times we see \textit{Heads} over these $n$ trials. Then, $X$ can take on any value between 0 and $n$; so if we prefer \textit{Heads} outcomes, then we would like to see higher values of $X$. Since we are using a fair coin, we expect the probability that we obtain \textit{Heads} on any given trial to be 0.5; likewise, the probability that we obtain \textit{Tails} on any given trial is 0.5, since events that occur with certainty have probability 1.

When $n = 1$, we have one coin toss or trial, and the probability of observing \textit{Heads} is $P(X = 1) = 0.5$, whereas the probability of observing \textit{Tails} is $P(X = 0) = 0.5$. In this special case, we say that $X$ is \href{https://en.wikipedia.org/wiki/Bernoulli_distribution}{Bernoulli}($p = 0.5)$-distributed. Here, the parameter $p = P(\{\textit{Heads}\}) = 0.5$ since the coin is fair; but if the coin were biased, then $p \not= 0.5$ and $P(\{\textit{Tails}\}) = 1 - p$, where $p$ is any real value between 0 and 1.

As in our case, trials in a sequence of coin tosses are independent of one another, so for example, the outcome observed in trial 1 has no effect on the outcome of any trial thereafter. If $n > 1$ and we denote the outcome of each trial by a Bernoulli($p)$-distributed random variable $X_i$, for $i = 1,...,n$, then we can define a new random variable $X := X_1 + ... + X_n$, representing the total number of \textit{Heads} observed over all $n$ trials. Now, we can make use of the fact that the sum of $n$ independent Bernoulli($p$)-distributed random variables is a \href{https://en.wikipedia.org/wiki/Binomial_distribution}{Binomial}($p$)-distributed random variable. Given that $X \sim$ Binomial($p)$, we can compute the probability that we observe exactly $k$ \textit{Heads}, $P(X = k) = \binom{n}{k} p^k(1-p)^{n-k}$, for any integer $k$ between 0 and $n$.

This formula arises from the  \href{https://en.wikipedia.org/wiki/Rule_of_product}{combinatorial multiplication principle} and the \href{https://en.wikipedia.org/wiki/Independence_(probability_theory)#Definition}{formal definition of independence}: two events are independent if the probability that both events occur is the product of the probabilities of the individual events, which is generalizable for $n$ events. For the \textit{extra} curious reader, this means $P(X = k) = \sum_{m = 1}^{\binom{n}{k}}\prod_{i \in A_m, j \not\in A_m}P(X_i = 1)P(X_j = 0) = (p^k(1-p)^{n-k} + ... + p^k(1-p)^{n-k}) = \binom{n}{k} p^k(1-p)^{n-k}$, where each $A_m$ is the set of $k$ indices corresponding to the $k$ trials with \textit{Heads} outcomes, $m = 1,..., \binom{n}{k}$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 2, Subsection 1(a) - Example END

\textbf{. . . . .}

It should now be clear why the coin toss experiment is similar to our problem. We have a sequence of independent trials (survey responses), each taking one of two values (\textit{Yes} or \textit{No}). Each survey response is like a coin flip, albeit most likely using a biased coin. This implies that each survey response could be represented by a Bernoulli random variable, and thus, a collection of $n$ of these surveys could be represented by a binomially distributed random variable. But how should we determine the parameter $p = P(\{\textit{Yes}\})$?

If we assume that \texttt{mass.gov} users are equally likely to submit survey responses whether or not they are able to find desired content, then the unknown parameter $p$ could be estimated by the proportion of users who are able to navigate through the appropriate set of pages to desired content without difficulty. Since we do not and cannot in practice know the true (population) value of $p$, we must settle for an estimate $\theta$ of $p$ - one which we believe is reasonably accurate. To do this, we use \href{https://en.wikipedia.org/wiki/Bayesian_inference}{Bayesian inference}.