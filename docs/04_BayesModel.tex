% Section 2, Subsection 2
\subsection*{Model Implementation with Formstack Data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 2, Subsection 2(a)

\textbf{The Main Idea \& Set Up}

The previous example should have elucidated the goals of Bayesian inference and the underlying concepts involved. We now describe Bayesian inference and \href{https://en.wikipedia.org/wiki/Credible_interval}{credible intervals} in relation to our goals with Formstack data. Recall that our goal is to determine a reasonable estimate for the proportion of users who are able to navigate to desired content on \texttt{mass.gov}. Glancing at the existing data, we may have a rough idea of what this proportion could be, but after observing more trials' outcomes, we can converge on a more reliable estimate. Thus, a Bayesian inference model would be useful for our purposes here.

Recall that the number of affirmative survey responses is binomially distributed. But we do not know the true parameter value, so we settle for a parameter estimate $\theta$ obtained by choosing \href{https://en.wikipedia.org/wiki/Credible_interval}{credible intervals}. We treat our binomial distribution parameter estimate $\theta$ as a random variable and try to find an interval in which we believe the true population parameter lies some desired percentage of the time, given a reasonably large number of trials. In other words, we want to find the values of $a, b$ such that $P(a \le \theta \le b)$ is the desired probability describing how often the true value that $\theta$ estimates lies between $a$ and $b$. This notion is similar in spirit to that of \href{https://en.wikipedia.org/wiki/Confidence_interval}{confidence intervals}, but is \href{https://en.wikipedia.org/wiki/Credible_interval#Contrasts_with_confidence_interval}{fundamentally different}. In particular, $b$ should be slightly larger than $a$ so that our interval is narrow. Since $\theta$ models the probability $P(\{\textit{Yes}\})$ or proportion of users affirming that they found desired content, we expect $\theta$ to be some real value in $[0,1]$. Throughout the redesign process, we strive for higher values of $\theta$.

Notice that we are treating $\theta$ as a variable entity when considering it as a random variable (with inherent probability distribution) for which we are working to find a credible interval in which we believe the true parameter value resides some desired percentage of the time. However, when viewed as a parameter value for the binomial distribution, $\theta$ is treated as a deterministic quantity. It might be confusing and conceptually contradictory to interpret $\theta$ as both a variable and deterministic quantity, but this methodology (as used in Bayesian inference models) has seen many successful and useful applications in practice.

Returning to our Bayesian model - treating $\theta$ as a \href{https://onlinecourses.science.psu.edu/stat414/node/88}{continuous random variable} in $[0,1]$, we set up our formula for each iteration (cohort survey responses) using Bayes' Theorem. Given a particular value $\theta = x$ in $[0,1]$, let $X|\theta \sim$ Binomial($\theta = x)$ be a random variable representing the number of affirmative survey responses. Note that this is a \href{https://en.wikipedia.org/wiki/Conditional_probability_distribution}{conditional distribution} given a value (very small range of values) for $\theta$. We make use of the notation $a \in A$, signifying that $a$ is an element of the set $A$. For example, $\theta \in A$ could be represented by the event $A = \{0.55 \le \theta \le 0.6\}$ or the event $A = \{0.3 \le \theta < 0.4\}$. Meanwhile, $a \not\in A$ signifies that $a$ is not an element of the set $A$. Given observations $X \in B$, we want the \texttt{posterior} probability that $\theta \in A$:
\begin{align*}
    P(\theta \in A | X \in B ) &= P(X \in B | \theta \in A) \cdot P(\theta \in A) / P(X \in B) \\
    &= \frac{P(X \in B | \theta \in A) \cdot P(\theta \in A)}{P(X \in B, \theta \in A) + P(X \in B, \theta \not\in A)} \\
    &= \frac{P(X \in B | \theta \in A) \cdot P(\theta \in A)}{P(X \in B | \theta \in A) \cdot P(\theta \in A) + P(X \in B | \theta \not\in A) \cdot P(\theta \not\in A)} \\
    \\
    \texttt{posterior } &\texttt{= } P(\theta \in A | X \in B ) \\
    \texttt{likelihood } &\texttt{= } P(X \in B | \theta \in A) \\
    \texttt{prior } &\texttt{= } P(\theta \in A) \\
    \texttt{evidence } &\texttt{= } P(X \in B) = P(X \in B | \theta \in A) \cdot P(\theta \in A) + P(X \in B | \theta \not\in A) \cdot P(\theta \not\in A) \\
    \\
    \texttt{posterior } &\texttt{= } \texttt{likelihood} \cdot \texttt{prior} / \texttt{evidence}
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 2, Subsection 2(b)

\textbf{Leveraging Conjugacy \& the Beta Distribution}

But how do we describe the distribution of random variable $\theta$? Based on the structure of our formula, it is clear that unless the \href{https://en.wikipedia.org/wiki/Probability_density_function}{probability density function} of $\theta$ is of the same functional form as the \href{https://en.wikipedia.org/wiki/Binomial_distribution#Probability_mass_function}{probability mass function of the binomial distribution}, our calculations could become messy. To address this possible issue, we conveniently make use of \href{https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions}{conjugate distributions}. In particular, since our \texttt{likelihood} values can be modeled by a binomial distribution for a given $\theta$, we let our random variable $\theta$ assume a \href{https://en.wikipedia.org/wiki/Beta_distribution}{beta distribution}.

This ensures that for each iteration of the algorithm, the product of our \texttt{likelihood} and \texttt{prior} is of the same algebraic form for ease of manipulation. Likewise, the \texttt{evidence} term will be of the same algebraic form. To see this, let $A = (x - \epsilon, x + \epsilon)$, for some real number $x \in [0,1]$ and some arbitrarily small real number $\epsilon > 0$. Recall that if we assume $X|\theta \sim$ Binomial($\theta = x)$, then \texttt{likelihood =} $P(X = k|\theta \in A) = \binom{n}{k}x^k(1-x)^{n-k}$, where $\binom{n}{k}$ is just a constant. Meanwhile, if we assume that $\theta \sim$ Beta($\alpha,\beta)$, for some positive real-valued shape parameters $\alpha$ and $\beta$, then we have \texttt{prior = }$P(\theta \in A) = f(x) = [\frac{(\alpha + \beta - 1)!}{(\alpha-1)!(\beta-1)!}]x^{\alpha-1}(1-x)^{\beta-1}$, where $[\frac{(\alpha + \beta - 1)!}{(\alpha-1)!(\beta-1)!}]$ is also just a constant. Now, it should be clear how the binomial and beta distributions have compatible algebraic forms. Taking the product \texttt{likelihood} $\cdot$ \texttt{prior} = $P(X = k|\theta \in A)P(\theta \in A) = [\binom{n}{k}\cdot\frac{(\alpha + \beta - 1)!}{(\alpha-1)!(\beta-1)!}]x^k(1-x)^{n-k}x^{\alpha-1}(1-x)^{\beta-1}$ simplifies to the form (\textit{constant})$\cdot x^{k+\alpha - 1}(1-x)^{n-k+\beta-1}$. Notice that the \texttt{evidence} term form follows easily from that of the \texttt{likelihood} $\cdot$ \texttt{prior} term.

Recall that the formula \texttt{posterior = likelihood $\cdot$ prior / evidence} implies the main principle and driving force of Bayesian inference, namely \texttt{posterior} $\propto$ \texttt{likelihood} $\cdot$ \texttt{prior}. Using this fact, we disregard the constant derived above, as well as the \texttt{evidence} term. This frees us to assume that the random variable $\theta|X$, which encodes iteration values of the \texttt{posterior} variable, may be described by a beta distribution, much like the random variable $\theta$ for the \texttt{prior} variable. However, the beta distribution originally assumed by $\theta$ (expressed by \texttt{prior} values) differs from that assumed by $\theta|X$ (expressed by \texttt{posterior} values) by the shape parameters $\alpha$ and $\beta$. But notice that since we are after the \texttt{posterior}, modeled by the beta-distributed random variable $\theta|X$, we need only determine the shape parameters $\alpha$ and $\beta$ for each iteration. Thus, by taking advantage of conjugacy, we save ourselves much time and computational effort. Though we are reducing the problem to assuming that \texttt{posterior} values can simply be modeled by $\theta|X \sim$ Beta($\alpha, \beta$), this method works very well in practice and has seen many theoretical advances.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 2, Subsection 2(c)

\textbf{Determining Beta Distribution Shape Parameters $\alpha, \beta$}

Assuming a beta distribution for $\theta$ (modeling \texttt{prior} values), we must find or choose suitable \href{https://en.wikipedia.org/wiki/Shape_parameter}{shape parameters} $\alpha$ and $\beta$, which determine the \href{https://en.wikipedia.org/wiki/Probability_distribution}{distribution graph} shape and underlying statistical measures, such as the \href{https://en.wikipedia.org/wiki/Expected_value}{mean}, \href{https://en.wikipedia.org/wiki/Median}{median}, and \href{https://en.wikipedia.org/wiki/Variance}{variance}. To see this, consult the beta distribution formulas for measures of \href{https://en.wikipedia.org/wiki/Beta_distribution#Measures_of_central_tendency}{central tendency} and \href{https://en.wikipedia.org/wiki/Beta_distribution#Measures_of_statistical_dispersion}{statistical dispersion}, as well as this \href{http://ctmckay.me/bayesian-explorer/}{beta-distributed prior visualization tool} created by Connor McKay. The beta distribution here has been reparametrized with \href{https://en.wikipedia.org/wiki/Beta_distribution#Mean}{mean} and \href{https://en.wikipedia.org/wiki/Beta_distribution#Mean_and_sample_size}{shape parameters sum (concentration)}.

In the top graph, the $x$-axis (labeled \textit{domain}) gives the possible values for $\theta \in [0,1]$, while the $y$-axis (labeled \textit{prob\_dens}) gives the \href{https://en.wikipedia.org/wiki/Probability_density_function}{probability density} values for $\theta$ at these $x$ values. To understand why these density values are not bounded above by 1, please see \href{http://math.stackexchange.com/questions/105455/how-can-a-probability-density-be-greater-than-one-and-integrate-to-one}{this page}. Below this graph, we have the graph of the \href{https://en.wikipedia.org/wiki/Cumulative_distribution_function}{cumulative distribution function}, displaying the values of $P(\theta \le x)$, for each $x \in [0,1]$, which are bounded between 0 and 1. Notice again that we have reparametrized the beta distribution with a desired mean $(\frac{\alpha}{\alpha + \beta})$ and concentration ($\alpha + \beta)$, making the choice of our beta distribution parameters more intuitive.

Depending on how we select this mean and concentration, we can make use of certain properties of the resultant beta distribution for our \texttt{prior}. For instance, the \href{https://en.wikipedia.org/wiki/Beta_distribution#Probability_density_function}{probability density formula for a beta distribution with shape parameters $\alpha, \beta = 1$} (i.e., mean 0.5 and concentration 2) is equivalent to the \href{https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)#Probability_density_function}{probability density formula for a uniform distribution over [0,1]}. In \href{http://www.obscureanalytics.com/wp-content/uploads/2012/07/UniformBeta.png}{this situation}, every possible outcome $\theta \in [0,1]$ has an equal chance of occurring, which is \textit{not} incredibly informative for us. On the other hand, setting \href{http://www.obscureanalytics.com/wp-content/uploads/2012/07/BimodalBeta.png}{$\alpha, \beta = 0.5$ (i.e., mean = 0.5, $\alpha + \beta = 1$)}, we would likely see a very small $\theta$ or a very large $\theta$, in which case, our resultant Binomial($\theta)$ distribution would be characterized by having either a very low or a very high probability of success (affirmative responses), respectively. We would probably be more interested in a narrower range of $\theta \in [0,1]$ over which the density is highest, such as in the case where \href{http://www.obscureanalytics.com/wp-content/uploads/2012/07/StrongCentralBeta.png}{$\alpha$ and $\beta$ are both higher values - a strongly informative prior}. If we believe that affirmative responses occur rather \textit{infrequently}, we might opt for \href{http://www.obscureanalytics.com/wp-content/uploads/2012/07/StrongLowSuccessBeta.png}{this sort of scenario}. Alternatively, if we believe that affirmative responses occur \textit{frequently}, we might opt for \href{http://www.obscureanalytics.com/wp-content/uploads/2012/07/StrongHighSuccessBeta.png}{this sort of scenario}. In particular, the \textit{higher} the concentration ($\alpha + \beta$), the narrower our region of highest probability for the value of $\theta$ over [0,1], i.e., the \textit{more confident} we are of the value of $\theta$.

Once we have selected appropriate parameters (mean and concentration) for the beta distribution modeling the \texttt{prior} values, we have all the tools needed to obtain the \texttt{posterior} values for each iteration of our Bayesian inference model. Recall that since \texttt{posterior $\propto$ likelihood $\cdot$ prior}, we assume that a beta distribution describes the \texttt{posterior} values across trials (cohort survey responses). In particular, the beta distribution describing the \texttt{posterior} values across trials has shape parameters $\alpha_{\texttt{posterior}} = k + \alpha_{\texttt{prior}}$ and $\beta_{\texttt{posterior}} = n - k + \beta_{\texttt{prior}}$, where $n$ is the number of sample responses collected and $k$ is the number of affirmative survey responses. To get a better sense of the effects of conjugacy with our \texttt{prior} and \texttt{likelihood} distribution assumptions, switch to the \textit{Posterior} option in Connor's \href{http://ctmckay.me/bayesian-explorer/}{Bayesian Explorer tool}.

Recall the goal of our model - we want to find a reasonable estimate $\theta$, which we achieve by finding a window of values in $[0,1]$ where we are reasonably certain holds the true value describing the probability of an affirmative survey response for a given page or set of pages. Assuming that \texttt{mass.gov} users are equally likely to submit a Formstack survey response regardless of whether or not desired content was found, this $\theta$ describes the proportion of users who are able to find desired content on the site.